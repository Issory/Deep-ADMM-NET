function [ E,grad ] = Back( img,constants,params,net)
% Authored by Rick~
%BACK 反向传播函数
%   输入input:
%   constants
%   params
%   net
%   img

%   输出output:
%   E
%   grad
N = size(net.x,1);

% Constants
F = constants.F;
P = constants.P;
y = constants.y;
% parameters
H = params.H;
D = params.D;
rho = params.rho;
eta = params.eta;
q = params.q;

x_gt = reshape(img,[],1);
E = norm(net.x(N,1)-x_gt)/norm(x_gt);
grad = Init_grad(N,net);
[B,w] = Init_temp_params(N,net);



L = 1;
M =1;


%E_2_x_3 = {(x_3-x_gt)/(norm(x_gt)*norm(x_3-x_gt))};
grad.x_grad(N,1) = {(net.x(N,1)-x_gt)/(norm(x_gt)*norm(net.x(N,1)-x_gt))};
if N==1
    return
end

for i =N-1:2 %4（3，2，1）《3》
fprintf('BP Reconstruction Layer %d\n',i);
tic;
% E gradient
% function [E_2_gamma_n,E_2_rho_n,E_2_beta_nMinus1,E_2_z_nMinus1] = ...
% ReconstructionLayerGradient(E_2_x_n,P,F,y,H_n,B_m,rho_n,Z_n_Minus_1,beta_n_Minus_1,L,M)
[grad.gamma_grad(i+1,1),grad.rho_grad(i+1,1),E_2_beta_nMinus1_first,E_2_z_n_first]=...
    ReconstructionLayerGradient(grad.x_grad(N,1),...
    P,F,y,H(i+1,1),B(i+1,1),rho(i+1,1),net.z(i,1),net.beta(i,1),L,M);
if i==N-1
    grad.gamma_beta(i,1) = E_2_beta_nMinus1_first;
    


%---Second Layer-------------------------
% BP Multiplier update Layer2-----
fprintf('BP MultiplierUpdate Layer %d\n',i-1);
tic;
% function [E_2_eta_n,E_2_beta_nMinus1,E_2_c_n,E_2_z_n] = MultiplierUpdateLayerGradient(E_2_beta_n,c_n,z_n,eta_n,L)
[grad.eta_grad(i,1),E_2_beta_nMinus1_second,E_2_c_n_first,E_2_z_second] = ...
    MultiplierUpdateLayerGradient(grad.gamma_beta(i,1),net.c(i,1),net.z(i,1),eta(i,1),1);
toc;
% BP MultiplierUpdate Layer 2 END-----

% BP Nonlinear Transform Layer 2-----
fprintf('BP NonlinearTransform Layer2\n');
tic;
% E gradient
grad.z_grad(i,1) = {cell2mat(E_2_z_n_first) + cell2mat(E_2_z_second)};
% use function to compute
% [E_2_q_n,E_2_beta_nMinus1,E_2_c_n] =  NonlinearTransformLayerGradient(E_2_z_n,c_n,beta_nMinus1,q,L)
[grad.q_grad(i,1),E_2_beta_nMinus1_third,E_2_c_n_second]= NonlinearTransformLayerGradient...
    (grad.z_grad(i,1),net.c(i,1),net.beta(i-1,1),q(i,1),1);
toc;
% BP Nonlinear Transform Layer 2 END-----

% BP Convolution Layer 2-----
fprintf('BP Convolution Layer2\n');
tic;
% E gradient
grad.c_grad(i,1) = {cell2mat(E_2_c_n_first)+ cell2mat(E_2_c_n_second)};
% function [E_2_w_n,c_n_2_x_n] = ConvolutionLayerGradient(E_2_c_n,B_m,D_n,L,M)
[grad.w_grad(i,1),c_n_2_x_n] = ConvolutionLayerGradient(grad.c_grad(i,1),B(i,1),D(i,1),1,1,net.x(i,1));
toc;
% BP Convolution Layer 2 END-----
toc;

grad.x_grad(i,1) = {cell2mat(c_n_2_x_n) * cell2mat(E_2_c_2)};

end

end

function [B,w] = Init_temp_params(N,net)
    vec_size = size(net.x(1,1),1);
    B = cell(N,1);
    w = cell(N,1);
    for i=1:N
        B(i,1) = {dctmtx(vec_size)};
        w(i,1) = 1/N;
    end
end

%初始化梯度
function init_grad = Init_grad(N,net)
vec_size = size(net.x(1,1),1);


init_grad = struct;
init_grad.gamma_grad = cell(N,1);
init_grad.eta_grad = cell(N,1);
init_grad.rho_grad = cell(N,1);
init_grad.q_grad = cell(N,1);
init_grad.w_grad = cell(N,1);

init_grad.x_grad = cell(N,1);
init_grad.z_grad = cell(N,1);
init_grad.beta_grad = cell(N,1);
init_grad.c_grad = cell(N,1);

for i = 1:N
    init_grad.gamma_grad(i,1) = 0;
    init_grad.eta_grad(i,1) = 0;
    init_grad.rho_grad(i,1) = 0;
    init_grad.q_grad(i,1) = zeros(10,1);
    init_grad.w_grad(i,1) = 0;

    init_grad.x_grad(i,1) = zeros(vec_size,1);
    init_grad.z_grad(i,1) = zeros(vec_size,1);
    init_grad.beta_grad(i,1) = zeros(vec_size,1);
    init_grad.c_grad(i,1) = zeros(vec_size,1);
end
end